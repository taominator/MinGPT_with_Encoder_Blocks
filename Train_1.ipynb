{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c6c92a3-b6a7-40de-808e-a0c29c904e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, IterableDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ecc4295-fa76-44af-a486-439b677fb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset(major_index_str, encoder, chunk_size=500000):\n",
    "    base_folder = 'D:/Machine_Learning/MinGPT/extracted_tar_openwebtext'  # Replace with your base directory\n",
    "    context_length = 1024\n",
    "\n",
    "    x_list = []  \n",
    "    y_list = []  \n",
    "\n",
    "    current_directory = os.getcwd()\n",
    "    save_path = os.path.join(current_directory, 'datasets', f'subset_{major_index_str}')\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path, exist_ok=True)   \n",
    "\n",
    "    chunk_count = 0\n",
    "\n",
    "    completed_folders = 0\n",
    "\n",
    "    for folder in sorted(os.listdir(base_folder)):\n",
    "        if not folder.startswith(f'urlsf_subset{major_index_str.zfill(2)}'):\n",
    "            continue\n",
    "\n",
    "        full_path = os.path.join(base_folder, folder)\n",
    "\n",
    "        for txt_file in os.listdir(full_path):\n",
    "            if txt_file.endswith('.txt'):\n",
    "                with open(os.path.join(full_path, txt_file), 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                    tokenized_text = encoder.encode(text)\n",
    "                    \n",
    "                    if len(tokenized_text) >= context_length:\n",
    "                        for start_idx in range(0, len(tokenized_text) - context_length + 1, context_length):\n",
    "                            chunk = tokenized_text[start_idx:start_idx + context_length]\n",
    "                            for i in range(1, len(chunk)):\n",
    "                                x_list.append(torch.tensor(chunk[:i], dtype=torch.long))\n",
    "                                y_list.append(torch.tensor(chunk[i], dtype=torch.long))\n",
    "                    else:\n",
    "                        for i in range(1, len(tokenized_text)):\n",
    "                            x_list.append(torch.tensor(tokenized_text[:i], dtype=torch.long))\n",
    "                            y_list.append(torch.tensor(tokenized_text[i], dtype=torch.long))\n",
    "                    \n",
    "                    # Save when reaching chunk_size\n",
    "                    if len(x_list) >= chunk_size:\n",
    "                        dataset_chunk = {'data': x_list, 'labels': y_list}\n",
    "                        torch.save(dataset_chunk, os.path.join(save_path, f'subset_{major_index_str}_chunk_{chunk_count}.pth'))\n",
    "                        \n",
    "                        # Reset lists and increment chunk count\n",
    "                        x_list = []\n",
    "                        y_list = []\n",
    "                        chunk_count += 1\n",
    "\n",
    "        completed_folders += 1\n",
    "        print(f\"\\rTotal completed: {completed_folders}     Folder name: {folder}\")\n",
    "\n",
    "    # Save any remaining data\n",
    "    if x_list:\n",
    "        dataset_chunk = {'data': x_list, 'labels': y_list}\n",
    "        torch.save(dataset_chunk, os.path.join(save_path, f'subset_{major_index_str}_chunk_{chunk_count}.pth'))\n",
    "    \n",
    "    print(f\"Chunks saved for subset {major_index_str}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "756baa99-8305-4429-9c4e-1fa57d8be6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkedTextDataset(IterableDataset):\n",
    "    def __init__(self, major_index_str):\n",
    "        self.major_index_str = major_index_str\n",
    "        current_directory = os.getcwd()\n",
    "        self.base_path = os.path.join(current_directory, 'datasets')\n",
    "        self.subset_folder = os.path.join(self.base_path, f'subset_{major_index_str}')\n",
    "        \n",
    "        # Figure out the number of chunks by counting files in the directory\n",
    "        self.num_chunks = sum(1 for file in os.listdir(self.subset_folder) if file.startswith(f'subset_{major_index_str}_chunk_') and file.endswith('.pth'))\n",
    "\n",
    "    def __iter__(self):\n",
    "        #print('hi')\n",
    "        for i in range(self.num_chunks):\n",
    "            #print('hi 2')\n",
    "            chunk = torch.load(os.path.join(self.subset_folder, f'subset_{self.major_index_str}_chunk_{i}.pth'))\n",
    "            x_data = chunk['data']\n",
    "            y_data = chunk['labels']\n",
    "            for x, y in zip(x_data, y_data):\n",
    "                #print(x, y)\n",
    "                yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d4c672-4205-4e61-a57a-55e123f7f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.bpe import BPETokenizer\n",
    "\n",
    "encoder = BPETokenizer().encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92829b6d-7bfe-4b28-a5f3-f9ead6fb90d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total completed: 1     Folder name: urlsf_subset00-1000_data.FullName\n",
      "Total completed: 2     Folder name: urlsf_subset00-100_data.FullName\n",
      "Total completed: 3     Folder name: urlsf_subset00-101_data.FullName\n",
      "Total completed: 4     Folder name: urlsf_subset00-102_data.FullName\n",
      "Total completed: 5     Folder name: urlsf_subset00-103_data.FullName\n",
      "Total completed: 6     Folder name: urlsf_subset00-104_data.FullName\n",
      "Total completed: 7     Folder name: urlsf_subset00-105_data.FullName\n",
      "Total completed: 8     Folder name: urlsf_subset00-106_data.FullName\n",
      "Total completed: 9     Folder name: urlsf_subset00-107_data.FullName\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocess_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m, in \u001b[0;36mprocess_subset\u001b[1;34m(major_index_str, encoder, chunk_size)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chunk)):\n\u001b[0;32m     33\u001b[0m             x_list\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mtensor(chunk[:i], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[1;32m---> 34\u001b[0m             y_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokenized_text)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "process_subset('0', encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ab80d77-b476-4b29-8bc0-db967ac20e47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Machine_Learning\\MinGPT\\datasets\\subset_0\n",
      "7\n",
      "Data 1 - Input: tensor([1532]), Target: 345\n",
      "Data 2 - Input: tensor([1532,  345]), Target: 2107\n",
      "Data 3 - Input: tensor([1532,  345, 2107]), Target: 10522\n",
      "Data 4 - Input: tensor([ 1532,   345,  2107, 10522]), Target: 290\n",
      "Data 5 - Input: tensor([ 1532,   345,  2107, 10522,   290]), Target: 389\n",
      "Data 6 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389]), Target: 20623\n",
      "Data 7 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623]), Target: 281\n",
      "Data 8 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281]), Target: 7283\n",
      "Data 9 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283]), Target: 1268\n",
      "Data 10 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268]), Target: 329\n",
      "Data 11 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329]), Target: 257\n",
      "Data 12 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257]), Target: 3215\n",
      "Data 13 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257,  3215]), Target: 1200\n",
      "Data 14 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257,  3215,  1200]), Target: 508\n",
      "Data 15 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257,  3215,  1200,   508]), Target: 468\n",
      "Data 16 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257,  3215,  1200,   508,   468]), Target: 587\n",
      "Data 17 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257,  3215,  1200,   508,   468,   587]), Target: 8197\n",
      "Data 18 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257,  3215,  1200,   508,   468,   587,  8197]), Target: 393\n",
      "Data 19 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257,  3215,  1200,   508,   468,   587,  8197,   393]), Target: 11119\n",
      "Data 20 - Input: tensor([ 1532,   345,  2107, 10522,   290,   389, 20623,   281,  7283,  1268,\n",
      "          329,   257,  3215,  1200,   508,   468,   587,  8197,   393, 11119]), Target: 4624\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "dataset = ChunkedTextDataset('0')  # Assuming '0' for subset 0 as an example\n",
    "print(dataset.subset_folder)\n",
    "print(dataset.num_chunks)\n",
    "\n",
    "# Fetch and print the first 20 elements\n",
    "for idx, (x, y) in enumerate(dataset):\n",
    "    if idx >= 20:\n",
    "        break\n",
    "    print(f\"Data {idx + 1} - Input: {x}, Target: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af953c9-92d9-41d4-9b63-6a4a1e2b3236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
